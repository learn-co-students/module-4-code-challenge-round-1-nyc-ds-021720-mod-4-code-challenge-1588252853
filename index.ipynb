{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module 4 Code Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code challenge is designed to test your understanding of the Module 4 material. It covers:\n",
    "\n",
    "* Principal Component Analysis\n",
    "* Clustering\n",
    "* Time Series\n",
    "* Natural Language Processing\n",
    "\n",
    "_Read the instructions carefully._ You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "The goal here is to demonstrate your knowledge. Showing that you know things about certain concepts is more important than getting the best model. You can use any libraries you want to solve the problems in the assessment. \n",
    "\n",
    "### Note on the short answer questions\n",
    "\n",
    "For the short answer questions, _please use your own words._ The expectation is that you have **not** copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, you should do your best to communicate yourself clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Principal Component Analysis [Suggested Time: 15 minutes]\n",
    "\n",
    "---\n",
    "\n",
    "In the first part of the code challenge, you'll apply the unsupervised learning technique of Principal Component Analysis to the wine dataset. \n",
    "\n",
    "We load the wine dataset for you in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.628447</td>\n",
       "      <td>1.081206</td>\n",
       "      <td>-0.652127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.841477</td>\n",
       "      <td>-1.003358</td>\n",
       "      <td>-1.517062</td>\n",
       "      <td>1.711448</td>\n",
       "      <td>-1.230771</td>\n",
       "      <td>0.333174</td>\n",
       "      <td>-0.641378</td>\n",
       "      <td>-1.070901</td>\n",
       "      <td>-0.518219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.540882</td>\n",
       "      <td>-0.612994</td>\n",
       "      <td>-1.427534</td>\n",
       "      <td>0.288180</td>\n",
       "      <td>-1.037487</td>\n",
       "      <td>-0.112585</td>\n",
       "      <td>-0.086751</td>\n",
       "      <td>-0.350476</td>\n",
       "      <td>-0.195036</td>\n",
       "      <td>-0.933495</td>\n",
       "      <td>0.346530</td>\n",
       "      <td>1.330768</td>\n",
       "      <td>-0.215063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.755657</td>\n",
       "      <td>-1.287031</td>\n",
       "      <td>-1.538306</td>\n",
       "      <td>-1.354445</td>\n",
       "      <td>2.294697</td>\n",
       "      <td>-0.573329</td>\n",
       "      <td>-0.156280</td>\n",
       "      <td>-0.112562</td>\n",
       "      <td>2.014532</td>\n",
       "      <td>-0.722384</td>\n",
       "      <td>0.432435</td>\n",
       "      <td>-0.398434</td>\n",
       "      <td>0.041960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.377877</td>\n",
       "      <td>-0.694972</td>\n",
       "      <td>1.747940</td>\n",
       "      <td>-1.152719</td>\n",
       "      <td>0.595936</td>\n",
       "      <td>0.501741</td>\n",
       "      <td>0.668135</td>\n",
       "      <td>-0.191866</td>\n",
       "      <td>-0.402183</td>\n",
       "      <td>-0.171658</td>\n",
       "      <td>0.561293</td>\n",
       "      <td>0.246586</td>\n",
       "      <td>0.470333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.803385</td>\n",
       "      <td>0.388952</td>\n",
       "      <td>-0.541355</td>\n",
       "      <td>-0.432270</td>\n",
       "      <td>-0.841477</td>\n",
       "      <td>0.271369</td>\n",
       "      <td>0.241029</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>0.685339</td>\n",
       "      <td>-1.296056</td>\n",
       "      <td>0.819008</td>\n",
       "      <td>0.960225</td>\n",
       "      <td>-1.473819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid       ash  alcalinity_of_ash  magnesium  \\\n",
       "0  0.628447    1.081206 -0.652127           0.000000  -0.841477   \n",
       "1 -0.540882   -0.612994 -1.427534           0.288180  -1.037487   \n",
       "2 -0.755657   -1.287031 -1.538306          -1.354445   2.294697   \n",
       "3  0.377877   -0.694972  1.747940          -1.152719   0.595936   \n",
       "4 -0.803385    0.388952 -0.541355          -0.432270  -0.841477   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0      -1.003358   -1.517062              1.711448        -1.230771   \n",
       "1      -0.112585   -0.086751             -0.350476        -0.195036   \n",
       "2      -0.573329   -0.156280             -0.112562         2.014532   \n",
       "3       0.501741    0.668135             -0.191866        -0.402183   \n",
       "4       0.271369    0.241029             -0.905609         0.685339   \n",
       "\n",
       "   color_intensity       hue  od280/od315_of_diluted_wines   proline  \n",
       "0         0.333174 -0.641378                     -1.070901 -0.518219  \n",
       "1        -0.933495  0.346530                      1.330768 -0.215063  \n",
       "2        -0.722384  0.432435                     -0.398434  0.041960  \n",
       "3        -0.171658  0.561293                      0.246586  0.470333  \n",
       "4        -1.296056  0.819008                      0.960225 -1.473819  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Relevant imports\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "wine = load_wine()\n",
    "X, y = load_wine(return_X_y=True)\n",
    "X = pd.DataFrame(X, columns=wine.feature_names)\n",
    "y = pd.Series(y)\n",
    "y.name = 'class'\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "scaler_1 = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler_1.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# Inspect the first five rows of the scaled dataset\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 2, 1,\n",
       "       0, 2, 1, 1, 0, 1, 0, 0, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0,\n",
       "       1, 2, 2, 1, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 0, 2, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 2, 1, 0, 0, 1, 2, 2, 0, 1, 2,\n",
       "       2, 2, 2, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 2, 1, 0, 2, 2, 0, 0, 2, 2,\n",
       "       2, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Fit PCA to the training data\n",
    "\n",
    "Call the PCA instance you'll create `wine_pca`. Set `n_components=0.9` and make sure to use `random_state = 42`.\n",
    "\n",
    "_Make sure you are using the **preprocessed data!**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
       "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
       "       'proanthocyanins', 'color_intensity', 'hue',\n",
       "       'od280/od315_of_diluted_wines', 'proline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) How many principal components are there in the fitted PCA object?\n",
    "\n",
    "_Hint: Look at the list of attributes of trained `PCA` objects in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 principle components in the fitted PCA object.  There is one PC for each of the 13 features in the X_train_scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['pc 1', 'pc 2', 'pc 3', 'pc 4', 'pc 5', 'pc 6', 'pc 7', 'pc 8', 'pc 9', 'pc 10', 'pc 11', 'pc 12', 'pc 13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc 1</th>\n",
       "      <th>pc 2</th>\n",
       "      <th>pc 3</th>\n",
       "      <th>pc 4</th>\n",
       "      <th>pc 5</th>\n",
       "      <th>pc 6</th>\n",
       "      <th>pc 7</th>\n",
       "      <th>pc 8</th>\n",
       "      <th>pc 9</th>\n",
       "      <th>pc 10</th>\n",
       "      <th>pc 11</th>\n",
       "      <th>pc 12</th>\n",
       "      <th>pc 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-3.006949</td>\n",
       "      <td>-0.440982</td>\n",
       "      <td>-0.841225</td>\n",
       "      <td>0.340377</td>\n",
       "      <td>-1.173705</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>0.495014</td>\n",
       "      <td>0.091746</td>\n",
       "      <td>-0.263673</td>\n",
       "      <td>0.493392</td>\n",
       "      <td>0.092763</td>\n",
       "      <td>-0.127521</td>\n",
       "      <td>-0.169167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.478492</td>\n",
       "      <td>1.990401</td>\n",
       "      <td>-0.689619</td>\n",
       "      <td>0.427103</td>\n",
       "      <td>-0.360292</td>\n",
       "      <td>-0.533502</td>\n",
       "      <td>-0.010762</td>\n",
       "      <td>0.274893</td>\n",
       "      <td>0.505878</td>\n",
       "      <td>0.901588</td>\n",
       "      <td>0.364217</td>\n",
       "      <td>0.550354</td>\n",
       "      <td>-0.110847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.406608</td>\n",
       "      <td>0.773276</td>\n",
       "      <td>-1.547031</td>\n",
       "      <td>-1.530376</td>\n",
       "      <td>1.808724</td>\n",
       "      <td>2.374277</td>\n",
       "      <td>-0.418017</td>\n",
       "      <td>0.092185</td>\n",
       "      <td>0.221991</td>\n",
       "      <td>0.167960</td>\n",
       "      <td>-0.677731</td>\n",
       "      <td>-0.211717</td>\n",
       "      <td>0.045758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.378122</td>\n",
       "      <td>-0.678679</td>\n",
       "      <td>0.637309</td>\n",
       "      <td>-1.015856</td>\n",
       "      <td>-0.617612</td>\n",
       "      <td>-0.429807</td>\n",
       "      <td>0.468740</td>\n",
       "      <td>-0.955370</td>\n",
       "      <td>-0.298254</td>\n",
       "      <td>-0.761944</td>\n",
       "      <td>-0.816736</td>\n",
       "      <td>-0.198198</td>\n",
       "      <td>-0.125637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.803241</td>\n",
       "      <td>2.298979</td>\n",
       "      <td>-0.213368</td>\n",
       "      <td>1.173377</td>\n",
       "      <td>0.473106</td>\n",
       "      <td>-0.132372</td>\n",
       "      <td>0.657271</td>\n",
       "      <td>0.212064</td>\n",
       "      <td>-0.445093</td>\n",
       "      <td>-0.496086</td>\n",
       "      <td>-0.378699</td>\n",
       "      <td>0.273602</td>\n",
       "      <td>-0.342536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.664532</td>\n",
       "      <td>1.148479</td>\n",
       "      <td>1.840976</td>\n",
       "      <td>0.277946</td>\n",
       "      <td>-0.628004</td>\n",
       "      <td>-1.479020</td>\n",
       "      <td>-0.034486</td>\n",
       "      <td>1.933755</td>\n",
       "      <td>-1.017013</td>\n",
       "      <td>-0.329967</td>\n",
       "      <td>-0.250888</td>\n",
       "      <td>-0.274544</td>\n",
       "      <td>-0.360172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>-0.260845</td>\n",
       "      <td>2.058208</td>\n",
       "      <td>-0.666437</td>\n",
       "      <td>0.450632</td>\n",
       "      <td>-0.467165</td>\n",
       "      <td>-0.167303</td>\n",
       "      <td>-0.319556</td>\n",
       "      <td>0.124863</td>\n",
       "      <td>0.516625</td>\n",
       "      <td>0.168734</td>\n",
       "      <td>-0.524184</td>\n",
       "      <td>0.732122</td>\n",
       "      <td>0.192458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>4.225384</td>\n",
       "      <td>-2.401430</td>\n",
       "      <td>-1.045321</td>\n",
       "      <td>0.630063</td>\n",
       "      <td>-0.944982</td>\n",
       "      <td>0.961456</td>\n",
       "      <td>-0.578379</td>\n",
       "      <td>0.274131</td>\n",
       "      <td>0.527176</td>\n",
       "      <td>-1.001796</td>\n",
       "      <td>0.014393</td>\n",
       "      <td>-0.085148</td>\n",
       "      <td>-0.098091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>-1.758083</td>\n",
       "      <td>1.561886</td>\n",
       "      <td>-0.135570</td>\n",
       "      <td>-0.159673</td>\n",
       "      <td>-1.331025</td>\n",
       "      <td>0.857865</td>\n",
       "      <td>-0.667470</td>\n",
       "      <td>0.460080</td>\n",
       "      <td>0.461993</td>\n",
       "      <td>0.219159</td>\n",
       "      <td>-0.863772</td>\n",
       "      <td>-0.343466</td>\n",
       "      <td>0.292538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>-0.093134</td>\n",
       "      <td>1.254564</td>\n",
       "      <td>0.838492</td>\n",
       "      <td>0.565299</td>\n",
       "      <td>0.472789</td>\n",
       "      <td>-0.827740</td>\n",
       "      <td>0.345878</td>\n",
       "      <td>-0.794984</td>\n",
       "      <td>-0.073415</td>\n",
       "      <td>0.694480</td>\n",
       "      <td>-0.358385</td>\n",
       "      <td>-0.028303</td>\n",
       "      <td>-0.273001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pc 1      pc 2      pc 3      pc 4      pc 5      pc 6      pc 7  \\\n",
       "0   -3.006949 -0.440982 -0.841225  0.340377 -1.173705  0.574376  0.495014   \n",
       "1    0.478492  1.990401 -0.689619  0.427103 -0.360292 -0.533502 -0.010762   \n",
       "2    1.406608  0.773276 -1.547031 -1.530376  1.808724  2.374277 -0.418017   \n",
       "3    1.378122 -0.678679  0.637309 -1.015856 -0.617612 -0.429807  0.468740   \n",
       "4    0.803241  2.298979 -0.213368  1.173377  0.473106 -0.132372  0.657271   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "119  1.664532  1.148479  1.840976  0.277946 -0.628004 -1.479020 -0.034486   \n",
       "120 -0.260845  2.058208 -0.666437  0.450632 -0.467165 -0.167303 -0.319556   \n",
       "121  4.225384 -2.401430 -1.045321  0.630063 -0.944982  0.961456 -0.578379   \n",
       "122 -1.758083  1.561886 -0.135570 -0.159673 -1.331025  0.857865 -0.667470   \n",
       "123 -0.093134  1.254564  0.838492  0.565299  0.472789 -0.827740  0.345878   \n",
       "\n",
       "         pc 8      pc 9     pc 10     pc 11     pc 12     pc 13  \n",
       "0    0.091746 -0.263673  0.493392  0.092763 -0.127521 -0.169167  \n",
       "1    0.274893  0.505878  0.901588  0.364217  0.550354 -0.110847  \n",
       "2    0.092185  0.221991  0.167960 -0.677731 -0.211717  0.045758  \n",
       "3   -0.955370 -0.298254 -0.761944 -0.816736 -0.198198 -0.125637  \n",
       "4    0.212064 -0.445093 -0.496086 -0.378699  0.273602 -0.342536  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "119  1.933755 -1.017013 -0.329967 -0.250888 -0.274544 -0.360172  \n",
       "120  0.124863  0.516625  0.168734 -0.524184  0.732122  0.192458  \n",
       "121  0.274131  0.527176 -1.001796  0.014393 -0.085148 -0.098091  \n",
       "122  0.460080  0.461993  0.219159 -0.863772 -0.343466  0.292538  \n",
       "123 -0.794984 -0.073415  0.694480 -0.358385 -0.028303 -0.273001  \n",
       "\n",
       "[124 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Is PCA more useful or less useful when you have high multicollinearity among your features? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is more useful when you have high multicolinearity because PCs are linear combinations that summarize the variance of the features.  Multicolineary occurs when features are correlated with each other.  However, each PC is completely independent of the other PCs and so this reduces multicolinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Part 2: Clustering [Suggested Time: 20 minutes]\n",
    "\n",
    "---\n",
    "\n",
    "This second part of the code challenge is meant to test your clustering knowledge.\n",
    "\n",
    "* If the gif doesn't run, you may access it via [this link](images/centroid.gif).\n",
    "\n",
    "<img src='images/centroid.gif'>\n",
    "\n",
    "### 2.1) Using the gif above for reference, describe the steps of the k-means clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Determine the number of clusters (K) you want.  K-means is a top down approach to clustering so you need to decide how many centroids you want before the algorithm can run.\n",
    "\n",
    "Step 2: K number of seeds are randomly placed among the data.\n",
    "\n",
    "Step 3: Observations are assigned (clustered) to the randomly placed centroids that are nearest using a specific distance metric like Euclidean distance.\n",
    "\n",
    "Step 3: The mean distance of each cluster is calculated and a new centroid is positioned at the mean distance of each cluster.\n",
    "\n",
    "Step 4: Observations are reassigned to these new centroids based on which is nearest.\n",
    "\n",
    "Step 5: Repeat step 3 and 4 until the clusters can no longer be updated - meaning the mean of each cluster remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the wine dataset again, this time for clustering.\n",
    "\n",
    "You will use scikit-learn to fit k-means clustering models, and you will determine the optimal number of clusters to use by looking at silhouette scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Write a function called `get_labels()` that will find `k` clusters in a dataset of features `X`, and return the labels for each row of `X`. \n",
    "\n",
    "_Hint: Within the function, you'll need to:_\n",
    "* instantiate a k-means clustering model (use `random_state = 1` for reproducibility),\n",
    "* fit the model to the data, and\n",
    "* return the labels for each point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'labels_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eb2904cd35ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_km\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_km\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'labels_'"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=k, init='random', random_state=1)\n",
    "km = kmeans.fit(X)\n",
    "y_km = km.predict(X)\n",
    "y_km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Relevant import(s) here\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_labels(k, X):\n",
    "    \"\"\" \n",
    "    Finds the labels from a k-means clustering model \n",
    "    \n",
    "    Parameters: \n",
    "    -----------\n",
    "    k: float object\n",
    "        number of clusters to use in the k-means clustering model\n",
    "    X: Pandas DataFrame or array-like object\n",
    "        Data to cluster\n",
    "    \n",
    "    Returns: \n",
    "    --------\n",
    "    labels: array-like object\n",
    "        Labels attribute from the k-means model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate a k-means clustering model with random_state=1 and n_clusters=k\n",
    "    kmeans = KMeans(n_clusters=k, init='random', random_state=1)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    km = kmeans.fit(X)\n",
    "    y_km = km.predict(X)\n",
    "    \n",
    "    # Return the predicted labels for each row in the data produced by the model\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we fit the k-means algorithm to the wine data for $k$ values in the range 2 to 9 using the function you've written above. Then we obtain the silhouette scores for each trained k-means clustering model, and place the values in a list called `silhouette_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Preprocessing is needed. Scale the data\n",
    "scaler_2 = StandardScaler()\n",
    "X_scaled = scaler_2.fit_transform(X)\n",
    "\n",
    "# Create empty list for silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Range of k values to try\n",
    "k_values = range(2, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    labels = get_labels(k, X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels, metric='euclidean')\n",
    "    silhouette_scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the silhouette scores obtained for each different value of $k$, against $k$, the number of clusters we asked the algorithm to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.title('Silhouette scores vs number of clusters')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('silhouette score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Which value of $k$ would you choose based on the above plot of silhouette scores? How does this number compare to the number of classes in the [wine dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)?\n",
    "\n",
    "Hint: this number should be <= 5. If it's not, check the function written for Question 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best number of clusters is 3 based on the silhouette score.  The highest silhouette score is the best, which signifies the number of K that results in the tightest clusters and also the greatest distance between clusters.  This is accurate considering there are 3 classes in the wine dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Natural Language Processing [Suggested Time: 20 minutes]\n",
    "\n",
    "---\n",
    "\n",
    "In this third section we will attempt to classify text messages as \"SPAM\" or \"HAM\" using TF-IDF Vectorization. Once we successfully classify our texts we will consider how to interpret the vectorization.\n",
    "\n",
    "Complete the functions below and answer the question at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matthewflavell/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Import necessary libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords') # un-comment this if you get an error from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  target\n",
       "0   ham  Go until jurong point, crazy.. Available only ...       0\n",
       "1   ham                      Ok lar... Joking wif u oni...       0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3   ham  U dun say so early hor... U c already then say...       0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Read in data\n",
    "df_messages = pd.read_csv('data/spam.csv', usecols=[0,1])\n",
    "\n",
    "# Convert string labels to 1 or 0 \n",
    "le = LabelEncoder()\n",
    "df_messages['target'] = le.fit_transform(df_messages['v1'])\n",
    "\n",
    "# Examine our data\n",
    "df_messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Separate features and labels \n",
    "X = df_messages['v2']\n",
    "y = df_messages['target']\n",
    "\n",
    "# Generate a list of stopwords \n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Create a function that takes in our various texts along with their respective labels and uses TF-IDF to vectorize the texts.\n",
    "\n",
    " - Recall that TF-IDF helps us \"vectorize\" text (turn text into numbers) so we can do \"math\" with it.  It is used to quantify how relevant a term is in a given document.\n",
    " - **DO NOT** perform tokenization, removal of stop words, or TF-IDF vectorization \"by hand\".  Use `sklearn`'s `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"pass\" with appropriate code\n",
    "\n",
    "def tfidf(X, y, stopwords_list): \n",
    "    \"\"\"\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    stopwords_list: list ojbect\n",
    "        List containing words and punctuation to remove. \n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vectorizer object\n",
    "\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    tf_idf_test = vectorizer.fit_transform(X_test)\n",
    "    \n",
    "    return tf_idf_train, tf_idf_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "tf_idf_train, tf_idf_test, y_train, y_test, vectorizer = tfidf(X, y, stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Complete the function below to return a list of predictions for our training set and a separate list of predictions for our test set.\n",
    "\n",
    "Now that we have a set of vectorized training data we can use this data to train a _classifier_ to learn how to classify a specific text based on the vectorized version of the text. Below we have initialized a simple Naive Bayes Classifier and Random Forest Classifier. \n",
    "\n",
    "The function should accept a classifier object, a vectorized training set, vectorized test set, and a list of training labels to return separate lists of predictions for the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 7398)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1393, 4044)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "nb_classifier = MultinomialNB()\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    \"\"\"\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    \"\"\"\n",
    "    train_df = pd.DataFrame.sparse.from_spmatrix(tf_idf_train)\n",
    "    \n",
    "    # Fit the classifier with our training data\n",
    "    clf = classifier.fit(train_df, y_train)\n",
    "    \n",
    "    # Predict the labels of our train data and store them in train_preds\n",
    "    train_preds = clf.predict(train_df)\n",
    "    \n",
    "    # Predict the labels of our test data and store them in test_preds\n",
    "    test_df = pd.DataFrame.sparse.from_spmatrix(tf_idf_test)\n",
    "    test_preds = clf.predict(test_df)\n",
    "    \n",
    "    return train_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and evaluate predictions for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 7398 is different from 4044)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-7d3aab81bf2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell without changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_test_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-91a5b30e46d3>\u001b[0m in \u001b[0;36mclassify_text\u001b[0;34m(classifier, tf_idf_train, tf_idf_test, y_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Predict the labels of our test data and store them in test_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_spmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;34m\"\"\"Calculate the posterior log probability of the samples X\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    771\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 7398 is different from 4044)"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "nb_train_preds, nb_test_preds = classify_text(nb_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, nb_test_preds))\n",
    "print(accuracy_score(y_test, nb_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and evaluate predictions for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 7398 and input n_features is 4044 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f7eff97d7fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell without changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf_train_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_test_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_test_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-4e4258454c25>\u001b[0m in \u001b[0;36mclassify_text\u001b[0;34m(classifier, tf_idf_train, tf_idf_test, y_train)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Predict the labels of our test data and store them in test_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    389\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 7398 and input n_features is 4044 "
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier, tf_idf_train, tf_idf_test, y_train)\n",
    "\n",
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see both classifiers do a pretty good job classifying texts as either \"SPAM\" or \"HAM\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Based on the code below, the word \"genuine\" has the highest TF-IDF value in the second document of our test data. What does that tell us about the word \"genuine\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "tf_idf_test_df = pd.DataFrame(tf_idf_test.toarray(), columns=vectorizer.vocabulary_.keys())\n",
    "second_doc = tf_idf_test_df.loc[1]\n",
    "second_doc.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "second_doc['genuine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the word \"genuine\" occurs the least frequenctly in the second document of the test data.  TF-IDF is calculated by multiplying the Term Frequency by the Inverse Document Frequency.  The inverse document frequency is calulated by dividing the total number of documents by the number of words across all documents.  A high TF-IDF score means that the word is rare and therefore possibly more relevant than a word with a low TF-IDF score which means its more frequent and possibly less relevant/ meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Time Series [Suggested Time: 20 minutes]\n",
    "\n",
    "---\n",
    "\n",
    "<!---Create stock_df and save as .pkl\n",
    "stocks_df = pd.read_csv(\"raw_data/all_stocks_5yr.csv\")\n",
    "stocks_df[\"clean_date\"] = pd.to_datetime(stocks_df[\"date\"], format=\"%Y-%m-%d\")\n",
    "stocks_df.drop([\"date\", \"clean_date\", \"volume\", \"Name\"], axis=1, inplace=True)\n",
    "stocks_df.rename(columns={\"string_date\": \"date\"}, inplace=True)\n",
    "pickle.dump(stocks_df, open(\"write_data/all_stocks_5yr.pkl\", \"wb\"))\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you'll be looking at OHLC (Open, High, Low, Close) daily stock data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>February 08, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>February 11, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>February 12, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>February 13, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>February 14, 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    open   high    low  close               date\n",
       "0  15.07  15.12  14.63  14.75  February 08, 2013\n",
       "1  14.89  15.01  14.26  14.46  February 11, 2013\n",
       "2  14.45  14.51  14.10  14.27  February 12, 2013\n",
       "3  14.30  14.94  14.25  14.66  February 13, 2013\n",
       "4  14.94  14.96  13.16  13.99  February 14, 2013"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "stocks_df = pickle.load(open('write_data/all_stocks_5yr.pkl', 'rb'))\n",
    "stocks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Transform the `date` feature so that it becomes a `datetime` object, and set `date` to be the index of `stocks_df`.\n",
    "\n",
    "The format of the `date` feature is `'%B %d, %Y'` . Use this when converting the `date` feature to a `datetime` object in order for the code to run faster.\n",
    "\n",
    "Be sure that the `date` index of `stocks_df` is in the format: YYYY-MM-DD (should do so automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df['date'] = pd.to_datetime(stocks_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 619040 entries, 2013-02-08 to 2018-02-07\n",
      "Data columns (total 4 columns):\n",
      "open     619029 non-null float64\n",
      "high     619032 non-null float64\n",
      "low      619032 non-null float64\n",
      "close    619040 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 23.6 MB\n"
     ]
    }
   ],
   "source": [
    "stocks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-02-08', '2013-02-11', '2013-02-12', '2013-02-13',\n",
       "               '2013-02-14', '2013-02-15', '2013-02-19', '2013-02-20',\n",
       "               '2013-02-21', '2013-02-22',\n",
       "               ...\n",
       "               '2018-01-25', '2018-01-26', '2018-01-29', '2018-01-30',\n",
       "               '2018-01-31', '2018-02-01', '2018-02-02', '2018-02-05',\n",
       "               '2018-02-06', '2018-02-07'],\n",
       "              dtype='datetime64[ns]', name='date', length=619040, freq=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Downsample `stocks_df` using the mean of the `open`, `high`, `low`, and `close` features on a monthly basis. Store the results in `stocks_monthly_df`.\n",
    "\n",
    "Hint: `stocks_monthly_df` should have 61 rows and 4 columns after you perform downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "stocks_monthly_df = stocks_df.resample('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-e01b4a5e2231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this cell without changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstocks_monthly_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "stocks_monthly_df.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Create a line graph that visualizes the monthly open stock prices from `stocks_monthly_df`.\n",
    "\n",
    "This is for the purposes of identifying if average monthly open stock price is stationary or not, using the rolling mean and rolling standard deviation.\n",
    "\n",
    "Store a sliced version of `stocks_monthly_df` which grabs the `open` column in a new object called `open_monthly_series`.\n",
    "\n",
    "Hint: use a window size of 3 to represent one quarter of a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "open_monthly_series = None\n",
    "\n",
    "roll_mean = None\n",
    "roll_std = None\n",
    "\n",
    "# Note: do not rename the variables otherwise the plot code will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "fig, ax = plt.subplots(figsize=(13, 10))\n",
    "ax.plot(open_monthly_series, color='blue',label='Average monthly opening stock price')\n",
    "ax.plot(roll_mean, color='red', label='Rolling quarterly mean')\n",
    "ax.plot(roll_std, color='black', label='Rolling quarterly std. deviation')\n",
    "ax.set_ylim(0, 120)\n",
    "ax.legend()\n",
    "fig.suptitle('Average monthly open stock prices, Feb. 2013 to Feb. 2018')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your visual inspection of the above graph, is the monthly open stock price stationary? Explain your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Your written answer here\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4) Use the Dickey-Fuller test to identify if `open_monthly_series` is stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant import(s) here \n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this confirm your answer from Question 4.3? Explain why the time series is stationary or not based on the output from the Dickey-Fuller test. What is the null hypothesis, and were you able to reject it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Your written answer here\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
